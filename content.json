{"meta":{"title":"Welcome to my blog!","subtitle":"blogs","description":"","author":"CodeMajic","url":"http://codemajic.cn","root":"/"},"pages":[{"title":"关于","date":"2020-10-10T12:34:29.181Z","updated":"2020-07-06T12:45:10.000Z","comments":true,"path":"about/index.html","permalink":"http://codemajic.cn/about/index.html","excerpt":"","text":"这是我的个人主页"},{"title":"contact","date":"2020-03-14T09:09:57.000Z","updated":"2020-03-14T17:11:30.000Z","comments":true,"path":"contact/index.html","permalink":"http://codemajic.cn/contact/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2020-10-10T12:34:29.181Z","updated":"2020-07-06T12:46:38.000Z","comments":true,"path":"categories/index.html","permalink":"http://codemajic.cn/categories/index.html","excerpt":"","text":""},{"title":"Friends","date":"2020-10-10T12:34:29.182Z","updated":"2020-10-10T06:08:32.000Z","comments":true,"path":"friends/index.html","permalink":"http://codemajic.cn/friends/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-03-22T06:59:00.000Z","updated":"2020-04-04T08:30:00.000Z","comments":false,"path":"link/index.html","permalink":"http://codemajic.cn/link/index.html","excerpt":"","text":""},{"title":"","date":"2020-10-10T12:34:29.195Z","updated":"2020-07-06T12:49:14.000Z","comments":true,"path":"mylist/index.html","permalink":"http://codemajic.cn/mylist/index.html","excerpt":"","text":""},{"title":"music","date":"2020-03-22T07:49:17.000Z","updated":"2020-03-22T07:49:18.000Z","comments":true,"path":"music/index.html","permalink":"http://codemajic.cn/music/index.html","excerpt":"","text":""},{"title":"movies","date":"2020-03-22T07:49:32.000Z","updated":"2020-03-22T07:49:32.000Z","comments":true,"path":"movies/index.html","permalink":"http://codemajic.cn/movies/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-10-10T12:34:29.196Z","updated":"2020-07-06T12:47:26.000Z","comments":true,"path":"tags/index.html","permalink":"http://codemajic.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"RNN","slug":"RNN","date":"2020-04-09T06:55:21.000Z","updated":"2020-10-10T07:01:16.000Z","comments":true,"path":"post/4514.html","link":"","permalink":"http://codemajic.cn/post/4514.html","excerpt":"RNN 循环神经网络","text":"RNN 循环神经网络 循环神经网络 基本循环神经网络 网络组成：输入层、隐藏层、输出层。简单示意图如下 UUU ：输入层到输出层权重矩阵 VVV ：隐藏层到输出层权重矩阵 WWW ：隐藏层上一次的值作为当前t的输入矩阵 展开图： 假设ttt时刻，计算方法如下式： o_t=g(Vs_t)$$ （1） $S_t=f(Ux_t+Ws_{t-1})$ （2） 其中$g、f$为激活函数 **若反复将（2）式代入（1）式可得：** $$o_t=g(Vs_t)=g(Vf(Ux_t+Ws_{t-1}))=g(Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2})) 由上可以看出，在循环神经网络中输出值oto_tot​受前面历次xt,xt−1,xt−2,xt−3,...x_t,x_{t-1},x_{t-2},x_{t-3},...xt​,xt−1​,xt−2​,xt−3​,...输入值的影响，可以往前看任意多输入值，解决了在ngram语言模型中，受有限上文数量的约束。 总结：解决了ngram中有限上文信息的约束，同时存在只能往前看不能往后看的问题，在自然语言中上下文对词的影响具有相同的作用，无法捕捉长距离依赖信息。 双向循环神经网络 对于语言模型来说，只能够考虑上文得到的信息是不够的，应该同时兼顾上下文，提出了双向循环神经网络。 简单示意图： 在简单循环网络中，输出层的值由当前时刻的输入和上一时刻更新后的隐藏层权重参数共同决定，且隐藏层的权重参数为按照时间顺序进行传递。 由上图可以看出，在双向循环神经网路中，隐藏层权重参数AAA参与正向计算，A′A&#x27;A′参与反向计算，最终的输出结果由两项共同决定，以y2y_2y2​时刻为例，可得如下式： y2=g(VA2+V′A2′)y_2=g(VA_2+V&#x27;A&#x27;_2)y2​=g(VA2​+V′A2′​) A2=f(WA1+Ux2)A_2=f(WA_1+Ux_2)A2​=f(WA1​+Ux2​) A2′=f(W′A3′+U′x2)A&#x27;_2=f(W&#x27;A&#x27;_3+U&#x27;x_2)A2′​=f(W′A3′​+U′x2​) 由上述式可以看出最终的输出结果由前后共同决定，参照上一节形式给出双向循环神经网络的计算公式： ot=g(Vst+V′st′)o_t=g(Vs_t+V&#x27;s&#x27;_t) ot​=g(Vst​+V′st′​) st=f(Uxt+Wst−1)s_t=f(Ux_t+Ws_{t-1})st​=f(Uxt​+Wst−1​) st′=f(U′xt′+W′st+1′)s&#x27;_t=f(U&#x27;x&#x27;_t+W&#x27;s&#x27;_{t+1})st′​=f(U′xt′​+W′st+1′​) 总结：在双向循环神经网络中，解决了简单的循环神经网络中只能依赖上文信息的约束，采用双向传递方式对最后的结果进行计算。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://codemajic.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://codemajic.cn/tags/RNN/"}]},{"title":"Language Model","slug":"Language Model","date":"2020-04-08T04:55:21.000Z","updated":"2020-10-10T07:00:28.000Z","comments":true,"path":"post/36da.html","link":"","permalink":"http://codemajic.cn/post/36da.html","excerpt":"这是测试阅读全文","text":"这是测试阅读全文 NLP之语言模型（Language Model） 1、定义 标准定义：对于语言序列w1,w2,...,wnw_1,w_2,...,w_nw1​,w2​,...,wn​语言模型就是计算该序列的概率，即P(w1,w2,...,wn)P(w_1,w_2,...,w_n)P(w1​,w2​,...,wn​) 在机器学习角度：语言模型是对语句的概率分布的建模 简单解释：判断给定的语言序列是否是正常的语句，即是否符合日常交流的自然语言，通过计算两种语言序列的概率大小进行确定，PPP(She is a girl) &gt; PPP (He is a girl) 语言模型应该具备的功能: 对词具备基本的编码能力 有效表征语言的语法和语义特性，即对不同的语法和语义在对于的词向量上能够区分 对语言中包含的上下文多态性，对多义词要具备编码能力，且对不同上下文环境下的词在编码上能够区分 2、统计语言模型 2.1 背景知识 链式法则（Chain rule） P(w1,w2,...,wn)P(w_1,w_2,...,w_n)P(w1​,w2​,...,wn​)= P(w1)P(w2∣w1)⋅⋅⋅⋅P(wn∣w1,...,wn−1)P(w_1)P(w_2|w_1)····P(w_n|w_1,...,w_{n-1})P(w1​)P(w2​∣w1​)⋅⋅⋅⋅P(wn​∣w1​,...,wn−1​) 通过极大似然估计在训练数据中进行统计能够得到上述概率 存在问题：1、对任意长度的自然语言语句，计算上述概率不现实。2、训练数据不够多的情况下，会对没有出现的序列和未登录词按0进行处理 针对上述问题，引入了马尔可夫假设(Markov assumption)，假设当前词出现的概率只依赖于前n-1个词，即给定一个词，当前词出现的概率只依赖于前n−1n-1n−1个词。 P(wi∣w1,...,wi−1)=P(wi∣wi−n+1,...,wi−1)P(w_i|w_1,...,w_{i-1})=P(w_i|w_{i-n+1},...,w_{i-1})P(wi​∣w1​,...,wi−1​)=P(wi​∣wi−n+1​,...,wi−1​) 2.2 N-gram模型 基于上述背景知识，定义n-gram语言模型： n=1, P(w1,w2,...,wn)=∏i=1nP(wi)=P(w1)P(w_1,w_2,...,w_n)=\\prod_{i=1}^nP(w_i)=P(w_1)P(w1​,w2​,...,wn​)=∏i=1n​P(wi​)=P(w1​) n=2, P(w1,w2,...,wn)=∏i=1nP(wi∣wi−1)=P(w1)P(w2∣w1)P(w_1,w_2,...,w_n)=\\prod_{i=1}^nP(w_i|w_{i-1})=P(w_1)P(w_2|w_1)P(w1​,w2​,...,wn​)=∏i=1n​P(wi​∣wi−1​)=P(w1​)P(w2​∣w1​) n=3, P(w1,w2,...,wn)=∏i=1nP(wi∣wi−1,wn−2)=P(w1)P(w2∣w1)P(w3∣w2,w1)P(w_1,w_2,...,w_n)=\\prod_{i=1}^nP(w_i|w_{i-1},w_{n-2})=P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)P(w1​,w2​,...,wn​)=∏i=1n​P(wi​∣wi−1​,wn−2​)=P(w1​)P(w2​∣w1​)P(w3​∣w2​,w1​) 思考：在给定语言序列中加入起始符&lt;s&gt;&lt;s&gt;&lt;s&gt;，用来表征句首词出现的概率，此时P(w1)P(w_1)P(w1​)和P(w1∣&lt;s&gt;)P(w_1|&lt;s&gt;)P(w1​∣&lt;s&gt;)具有完全不同的意义。 结束符的重要意义 当不加结束符时，n-gram 语言模型只能分别对所有固定长度的序列进行概率分布建模，而不是任意长度的序列。 对于使用极大似然估计进行概率计算的等式，只有在语言序列包含结束符的时候能够成立。 对于未登录词和训练数据中未出现序列的处理采用平滑技术 总结 优点： 采用极大似然估计，能够很容易的对参数进行训练 能够处理任意长度的文本，完全包含了前n-1个词的全部信息 具有很好的可解释性，直观容易理解 缺点： 缺乏长期依赖，只能考虑当前词的前n-1个词 当n增大，参数空间也会呈指数增长 难免会出现OOV问题 单纯的基于语言序列在训练数据中的出现次数进行统计，泛化能力差 3、神经网络语言模型 语言模型总体目的：给定一个语言序列，预测出现下一个词的概率，即 P(wi∣w1,...,wi−1)P(w_i|w_1,...,w_{i-1} )P(wi​∣w1​,...,wi−1​) 在基于Markov假设的n-gram模型中，实际都是对上述式子的近似，以及简化部分上下文信息，这有利于理解神经网络语言模型的本质。 3.1 分布式表征词向量和传统的词频统计词向量 传统词频统计one-hot类型 由语料能够得到一个词表 对于每一个词采用词表大小的向量表示，给定词在向量中词表位置处的值为1，向量中其余值为0，所以对于不同的词之间完全正交相关性为0。 能够达到对原始语料进行编码的目的，但是对于不同词之间的相似性，语法语意的表征能力非常弱。 分布式表征词向量 每个词仍然是定长的向量，且每个词向量都位于同一个向量空间中 词向量之间具有明显的空间相关性，相似的词聚集在某一空间区域中切距离较近 3.2 前馈神经网络语言模型（包含有限的前文信息，学习到长距离的依赖关系） 3.3 循环神经网络语言模型 3.4 word2vec语言模型(CBOW&amp;Skip-gram) NNLM存在的问题 只能处理定长的序列和n-gram一样 训练时间非常长 13年提出word2vec和开源工具 CBoW(Continuous Bag-of-Words Model) 基本思想：从一个词序列中扣掉一个词，用其上下文去预测这个词 Skip-gram 基本思想：输入特定词的词向量，输出该词的上下文词向量 3.5 GloVe（Global Vectors for Word Representation）(2014年) ​ 基于全局词频统计的词表征模型 核心概括：将一段词序列（一段文本、基因序列、音频序列），表示为所有词（文字、单基因碱基对、单波峰音频）和其所在上下文（可以是前序/后序/双序）组合，的条件概率的累乘，即联合条件概率。文本存在即概率，万物存在皆有概率。 基本假设：基于Ngram滑动窗口，彼此相似的词，相比于彼此不相似的词，在窗口中共同出现的概率要更高，语义的相似性可以通过窗口共现概率来体现。 Glove提出了Ngram窗口用词共现矩阵(word-word co-occurrence matrix) 的方式来提取和表征语料的词法和语义。 统计所有的ngram滑动窗口中，每个中心词和其他背景词出现次数的累加和（假设词库单词个数为n，则共现矩阵为n维方阵，词库中单词顺序一定，故为对称阵） 思考：是否可以由词共现矩阵得到词向量表示 上述两个模型对自然语言中面临的挑战是“多义词问题” 3.4 预训练思想 神经网络训练流程：基于后向传播算法(BP)，通过对网络模型参数进行随机初始化，使用BP算法利用优化算法对模型参数进行调整。 预训练：网络模型的参数不再是进行随机初始化，先使用某个任务进行参数训练得到一套参数，再利用这套参数作为语言模型中初始化的参数进行训练。 ELMo(Embeddings from Language Models) 相关论文：Deep Contextualized Word Representations，Semi-supervised sequence tagging with bidirectional language models 方法：预训练(pre-train unsupervised)+微调(fine-tune supervised task model) 无监督预训练方法用于提取词的语法、语义和上下文信息，并且能够对词进行编码，作为指定任务的有监督语言模型中的输入。 有监督的微调方法以预训练结果作为输入，针对不同的NLP任务进行不同的有监督训练。 OpenAI GPT 相关论文：Improving Language Understanding by Generative Pre-Training 目的：学习一个通用的预训练表示，使得能够在大量任务上进行应用。 BERT 相关论文：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 3.5 总结 4、语言模型评价指标 采用相对熵来衡量两个分布之间的相近程度","categories":[{"name":"NLP","slug":"NLP","permalink":"http://codemajic.cn/categories/NLP/"}],"tags":[{"name":"Language Model","slug":"Language-Model","permalink":"http://codemajic.cn/tags/Language-Model/"}]},{"title":"code","slug":"codetest","date":"2020-04-08T04:55:21.000Z","updated":"2020-10-10T07:01:16.000Z","comments":true,"path":"post/b2c4.html","link":"","permalink":"http://codemajic.cn/post/b2c4.html","excerpt":"这是代码测试文章","text":"这是代码测试文章 import math def seq_tag_score(self, feats, tags): \"\"\" 得到 gold_seq tag 的 score \"\"\" # Gives the score of a provided tag sequence score = Var(torch.Tensor([0]).cuda(CUDA_ID)) tags = torch.cat([torch.LongTensor([tag2ids_[START_TAG]]).cuda(CUDA_ID), tags]) for i, feat in enumerate(feats): # self.transitions[tags[i + 1], tags[i]] 实际得到的是从标签i到标签i+1的转移概率 # feat[tags[i+1]] # feat 是 step i 的输出结果，有５个值，对应 B, I, E, START_TAG, END_TAG, 取对应标签的值 score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]] score = score + self.transitions[tag2ids_[STOP_TAG], tags[-1]] return score","categories":[{"name":"tese","slug":"tese","permalink":"http://codemajic.cn/categories/tese/"}],"tags":[{"name":"coding","slug":"coding","permalink":"http://codemajic.cn/tags/coding/"}]}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://codemajic.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"NLP","slug":"NLP","permalink":"http://codemajic.cn/categories/NLP/"},{"name":"tese","slug":"tese","permalink":"http://codemajic.cn/categories/tese/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://codemajic.cn/tags/RNN/"},{"name":"Language Model","slug":"Language-Model","permalink":"http://codemajic.cn/tags/Language-Model/"},{"name":"coding","slug":"coding","permalink":"http://codemajic.cn/tags/coding/"}]}